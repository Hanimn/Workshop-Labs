# RAG Pipeline for Cyber Threat Intelligence

A comprehensive Retrieval-Augmented Generation (RAG) system that ingests curated Cyber Threat Intelligence (CTI) data, creates vector embeddings, and provides intelligent responses to cybersecurity investigation queries using Anthropic's Claude API.

## ğŸ¯ Project Overview

This hands-on lab demonstrates how RAG technology can enhance cybersecurity operations by:

- **Grounding LLM responses** in verified threat intelligence data
- **Providing up-to-date information** from live threat feeds
- **Supporting investigation workflows** with source-attributed insights
- **Visualizing TTPs** using integrated MITRE ATT&CK Navigator
- **Reducing hallucination** through evidence-based responses

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚â”€â”€â”€â–¶â”‚   Ingestion     â”‚â”€â”€â”€â–¶â”‚   Processing    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ STIX/TAXII    â”‚    â”‚ â€¢ Parsers       â”‚    â”‚ â€¢ Chunking      â”‚
â”‚ â€¢ MITRE ATT&CK  â”‚    â”‚ â€¢ Validators    â”‚    â”‚ â€¢ Embeddings    â”‚
â”‚ â€¢ CVE Feeds     â”‚    â”‚ â€¢ Normalizers   â”‚    â”‚ â€¢ Metadata      â”‚
â”‚ â€¢ Threat Reportsâ”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Query    â”‚â”€â”€â”€â–¶â”‚   RAG Pipeline  â”‚â”€â”€â”€â–¶â”‚   Response      â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ CLI Interface â”‚    â”‚ â€¢ Vector Search â”‚    â”‚ â€¢ Grounded Text â”‚
â”‚ â€¢ Web Interface â”‚    â”‚ â€¢ Claude API    â”‚    â”‚ â€¢ Source Attrs  â”‚
â”‚ â€¢ Jupyter NB    â”‚    â”‚ â€¢ Prompt Eng.   â”‚    â”‚ â€¢ TTP Visualization â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  ChromaDB       â”‚
                    â”‚  Vector Store   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.12+
- Git
- Anthropic API key

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/Hanimn/Workshop-Labs.git
   cd Workshop-Labs
   ```

2. **Set up virtual environment:**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment:**
   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

5. **Initialize the system:**
   ```bash
   python -m src.setup.init_system
   ```

## ğŸ“ Project Structure

```
â”œâ”€â”€ data/                    # Data storage
â”‚   â”œâ”€â”€ raw/                # Raw CTI data
â”‚   â”œâ”€â”€ processed/          # Cleaned and normalized data
â”‚   â””â”€â”€ embeddings/         # Vector database
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ ingestion/          # Data collection and parsing
â”‚   â”œâ”€â”€ processing/         # Data processing and embeddings
â”‚   â”œâ”€â”€ rag/                # RAG pipeline implementation
â”‚   â””â”€â”€ interfaces/         # User interfaces (CLI, web, etc.)
â”œâ”€â”€ notebooks/              # Jupyter notebooks for analysis
â”œâ”€â”€ config/                 # Configuration management
â”œâ”€â”€ tests/                  # Unit and integration tests
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example           # Environment template
â””â”€â”€ README.md              # This file
```

## ğŸ”§ Configuration

The system uses environment variables and YAML configuration files. Key settings:

```bash
# LLM Configuration
LLM_ANTHROPIC_API_KEY=your_api_key_here
LLM_MODEL_NAME=claude-3-sonnet-20240229

# Database Configuration
DB_COLLECTION_NAME=cti_documents
DB_EMBEDDING_MODEL=all-MiniLM-L6-v2

# RAG Configuration
RAG_RETRIEVAL_TOP_K=10
RAG_CHUNK_SIZE=1000
```

## ğŸ® Usage Examples

### Command Line Interface

```bash
# Query threat intelligence
python -m src.interfaces.cli "What TTPs does APT29 use against healthcare?"

# Batch processing
python -m src.interfaces.cli --batch queries.txt

# Export results
python -m src.interfaces.cli --query "CVE-2024-1234 exploits" --export results.json
```

### Jupyter Notebooks

```bash
# Start Jupyter
jupyter notebook notebooks/

# Available notebooks:
# - 01_Data_Ingestion.ipynb
# - 02_Vector_Database.ipynb  
# - 03_RAG_Pipeline.ipynb
# - 04_Investigation_Scenarios.ipynb
```

### Web Interface

```bash
# Start web application
streamlit run src/interfaces/web_app.py

# Features:
# - Interactive chat interface
# - MITRE ATT&CK Navigator integration
# - Query history and export
# - Investigation scenario templates
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run specific test categories
pytest tests/unit/          # Unit tests
pytest tests/integration/   # Integration tests

# Run with coverage
pytest --cov=src tests/
```

## ğŸ“Š Evaluation Metrics

The system tracks performance using:

- **Context Precision**: Relevance of retrieved documents
- **Faithfulness**: Response grounding in sources  
- **Answer Relevancy**: Response alignment with queries
- **Response Time**: Query processing speed
- **Source Attribution**: Citation accuracy

## ğŸ”’ Security Considerations

- **API Keys**: Stored securely in environment variables
- **Input Validation**: Query sanitization and length limits
- **Data Protection**: Encrypted storage of sensitive CTI
- **Access Control**: Role-based permissions (future enhancement)

## ğŸ› ï¸ Development

### Adding New Data Sources

1. Create parser in `src/ingestion/parsers/`
2. Add configuration in `config/settings.py`
3. Update data schema in `src/processing/schema.py`
4. Add tests in `tests/unit/ingestion/`

### Extending RAG Pipeline

1. Implement new retrieval strategy in `src/rag/retrieval/`
2. Add prompt templates in `src/rag/prompts/`
3. Update response processing in `src/rag/generation/`

## ğŸ“ˆ Roadmap

- [x] **Phase 1**: RAG CTI Pipeline foundation with Ollama integration âœ…
- [ ] **Phase 2**: Multi-language CTI support
- [ ] **Phase 3**: Graph-based threat analysis
- [ ] **Phase 4**: Real-time feed integration
- [ ] **Phase 5**: SIEM platform connectors
- [ ] **Phase 6**: Automated threat hunting workflows

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- [MITRE ATT&CK](https://attack.mitre.org/) for the threat framework
- [STIX/TAXII](https://oasis-open.github.io/cti-documentation/) for CTI standards
- [Anthropic](https://www.anthropic.com/) for Claude API
- [Ollama](https://ollama.com/) for local LLM deployment
- [GPT-OSS](https://ollama.com/library/gpt-oss) for open-source language model
- [ChromaDB](https://www.trychroma.com/) for vector database
- [LangChain](https://langchain.readthedocs.io/) for RAG framework

## ğŸ“ Support

For questions, issues, or contributions:

- ğŸ“§ Email: momeninia.hani@investcyber.com
- ğŸ› Issues: [GitHub Issues](https://github.com/Hanimn/Workshop-Labs/issues)